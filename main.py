import argparse
import random
import time
from pathlib import Path

from rich.progress import Progress

from utils import Crawler, log


def main() -> None:
    parser = argparse.ArgumentParser(description="ğŸ•¸ï¸ åŸºäºDrissionPageåº“çš„èµ·ç‚¹å°è¯´çˆ¬è™«ã€‚")
    parser.add_argument(
        "-m",
        "--mode",
        choices=["full", "range"],
        required=True,
        help='ä¸‹è½½æ¨¡å¼ï¼šé€‰æ‹© "full" ä¸ºå…¨æ–‡ä¸‹è½½ï¼Œé€‰æ‹© "range" ä¸ºèŒƒå›´ä¸‹è½½',
    )
    parser.add_argument("url", type=str, help="ç›®å½•é¡µçš„URL")

    # è¿™äº›å‚æ•°ä»…åœ¨'range'æ¨¡å¼ä¸‹éœ€è¦
    parser.add_argument(
        "-u",
        "--upper-bound",
        type=int,
        default=None,
        help='èŒƒå›´ä¸‹è½½çš„ä¸Šç•Œï¼ˆä»…å½“é€‰æ‹© "range" æ¨¡å¼æ—¶æœ‰æ•ˆï¼‰',
    )
    parser.add_argument(
        "-l",
        "--lower-bound",
        type=int,
        default=None,
        help='èŒƒå›´ä¸‹è½½çš„ä¸‹ç•Œï¼ˆä»…å½“é€‰æ‹© "range" æ¨¡å¼æ—¶æœ‰æ•ˆï¼‰',
    )
    args = parser.parse_args()

    if args.mode == "full":
        full_download(args.url)
    else:
        if args.upper_bound is None or args.lower_bound is None:
            parser.error("åœ¨èŒƒå›´æ¨¡å¼ä¸‹ï¼Œå¿…é¡»åŒæ—¶æä¾› --upper-bound å’Œ --lower-bound")
        range_donwload(args.url, args.lower_bound, args.upper_bound)


def save(name: str, content: str) -> None:
    path = Path(f"{name}.txt")
    path.write_text(content, "utf-8")


def full_download(url: str) -> None:
    crawler = Crawler()
    log.info("ğŸ‰ DrissionPageåˆå§‹åŒ–å®Œæ¯•")

    index = crawler.get_index(url)
    log.info(f"ğŸˆ æ­£åœ¨ä¸‹è½½ã€Š{index.name}ã€‹ï¼Œå…·æœ‰{len(index.chpts)}ç« èŠ‚çš„å°è¯´")

    chpts: list[str] = []
    with Progress() as progress:
        download = progress.add_task("ğŸ›» ä¸‹è½½ä¸­", total=len(index.chpts))
        try:
            for info in index.chpts:
                chpt = crawler.get_chpt(info.url)
                chpts.append(chpt)
                progress.advance(download)
                time.sleep(random.randint(5, 7))
        except Exception as e:
            log.error(e)
        finally:
            content = "\n".join(chpts)
            save(index.name, content)
            log.info("âœ¨ å°è¯´ä¿å­˜å®Œæ¯•")


def range_donwload(url: str, lower_bound: int, upper_bound: int) -> None:
    if lower_bound > upper_bound:
        lower_bound, upper_bound = upper_bound, lower_bound
    lower_bound -= 1  # æ›´åŠ ç¬¦åˆä¹ æƒ¯ç”¨æ³•

    crawler = Crawler()
    log.info("ğŸ‰ DrissionPageåˆå§‹åŒ–å®Œæ¯•")

    index = crawler.get_index(url)
    lower_name = index.chpts[lower_bound].name
    upper_name = index.chpts[upper_bound - 1].name
    log.info(f"ğŸˆ æ­£åœ¨ä¸‹è½½ã€Š{index.name}ã€‹ï¼ŒèŒƒå›´ä»ã€Š{lower_name}ã€‹åˆ°ã€Š{upper_name}ã€‹")

    chpts: list[str] = []
    with Progress() as progress:
        download = progress.add_task("ğŸ›» ä¸‹è½½ä¸­", total=upper_bound - lower_bound)
        try:
            for info in index.chpts[lower_bound:upper_bound]:
                chpt = crawler.get_chpt(info.url)
                chpts.append(chpt)
                progress.advance(download)
                time.sleep(random.randint(5, 7))
        except Exception as e:
            log.error(e)
        finally:
            content = "\n".join(chpts)
            save(f"{index.name}-{lower_bound + 1}-{upper_bound}", content)
            log.info("âœ¨ å°è¯´ä¿å­˜å®Œæ¯•")


if __name__ == "__main__":
    try:
        main()
    except Exception as e:
        log.error(e)
