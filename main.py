import atexit
import logging
import pickle
import random
import time

from dataclasses import dataclass
from os.path import exists

from DrissionPage import ChromiumOptions, ChromiumPage
from rich.logging import RichHandler
from rich.progress import Progress

log = logging.getLogger()


@dataclass
class Info:
    url: str
    name: str
    author: str
    urls: list[str]
    count: int  # ç›®å‰ä¸‹è½½åˆ°çš„ä½ç½®ï¼Œç”¨äºæ–­ç‚¹ç»­ä¼ ã€‚


class Crawler:
    def __init__(self, info: Info | None = None) -> None:
        self.info = info
        self.novel = []
        try:
            self.page = ChromiumPage()
        except FileNotFoundError:
            log.error("â— Drissionself.Pageæ²¡æœ‰æ‰¾åˆ°Chromeæµè§ˆå™¨")
            log.info("è¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š")
            log.info(
                "1. æ‰“å¼€æµè§ˆå™¨ï¼Œåœ¨åœ°å€æ è¾“å…¥chrome://versionï¼ˆEdge è¾“å…¥edge://versionï¼‰ï¼Œå›è½¦ã€‚"
            )
            log.info("2. å¤åˆ¶â€œå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„â€åçš„å€¼")
            log.info("3. ç²˜è´´å€¼è‡³æ­¤å¤„")
            path = input("å€¼ç²˜è´´å¤„ï¼š")
            ChromiumOptions().set_browser_path(path).set_retry(5, 5).save()
            self.page = ChromiumPage()

        log.info("ğŸ‰ DrissionPage åˆå§‹åŒ–æˆåŠŸ")

    def get_index(self, url: str) -> Info:
        self.page.get(url)
        name = self.page.ele("#bookName").text
        author = self.page.ele(".author").text
        elems = self.page.s_eles(".chapter-name")
        urls = []
        for elem in elems:
            href = elem.attr("href")
            if href is not None:
                urls.append(href)
        return Info(url, name, author, urls, 0)  # type: ignore

    def get_chpt(self, chpt: str) -> str:
        self.page.get(chpt)
        content = []
        title = self.page.ele(".:title").text
        content.append(title)
        for elem in self.page.eles(".content-text"):
            content.append(elem.text)
        return "\n".join(content)

    def download(self, url: str) -> None:
        atexit.register(self.save)
        log.info(f"ğŸ‰ æ­£åœ¨ä¸‹è½½å°è¯´!")
        if self.info is not None:
            name, author, urls, count = (
                self.info.name,
                self.info.author,
                self.info.urls,
                self.info.count,
            )
            log.info("â„¹ å·²è·å–å…ˆå‰ä¸‹è½½ä¿¡æ¯")
            log.info(f"ã€Š{name}ã€‹({author})ï¼Œ å·²ä¸‹è½½{count}ç« ")
        else:
            index = self.get_index(url)
            self.info = index
            name, author, urls, count = (
                index.name,
                index.author,
                index.urls,
                index.count,
            )
            self.novel.append(name)
            self.novel.append(author)
        with Progress() as pg:
            download = pg.add_task("ä¸‹è½½ä¸­", total=len(urls) - count)
            for i, url in enumerate(urls[count:]):
                chpt = self.get_chpt(url)
                self.novel.append(chpt)
                self.info.count = i + count
                pg.advance(download)
                time.sleep(random.randint(2, 5))

    def save(self) -> None:
        if self.info is None:
            return None
        name = self.info.name
        with open(f"{name}.txt", "a", encoding="utf-8") as f:
            f.write("\n".join(self.novel))
            log.info("âœ¨ ä¸‹è½½å®Œæ¯•")
        with open("temp.pkl", "wb") as f:
            pickle.dump(self.info, f)
            log.info("ğŸ“• ä¸‹è½½ä¿¡æ¯å·²ä¿å­˜")


def main() -> None:
    url = input("è¯·è¾“å…¥å°è¯´é“¾æ¥ï¼š")
    info = None
    if exists("temp.pkl"):
        with open("temp.pkl", "rb") as f:
            info = pickle.load(f)
    if isinstance(info, Info) and info.url == url:
        crawler = Crawler(info)
    else:
        crawler = Crawler()
    crawler.download(url)


if __name__ == "__main__":
    logging.basicConfig(
        level="INFO",
        format="%(message)s",
        datefmt="[%X]",
        handlers=[RichHandler(rich_tracebacks=True, tracebacks_show_locals=True)],
    )
    log = logging.getLogger("rich")

    try:
        main()
    except Exception as e:
        log.exception(e)
